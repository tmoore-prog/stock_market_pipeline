# ğŸ“ˆ Stock Market Analytics Pipeline

A production-grade data pipeline that ingests, transforms, and analyzes 5.3+ million rows of daily U.S. equity market data, focusing on Russell 3000 constituents. Built with modern data engineering best practices using Apache Airflow, dbt, and Google BigQuery.

## ğŸ—ï¸ Architecture Overview
```mermaid
graph LR
    A[Polygon API] -->|Daily Extract| B[Apache Airflow]
    B -->|Load| C[BigQuery Raw Layer]
    C -->|dbt Transform| D[Staging Layer]
    D --> E[Intermediate Layer]
    E --> F[Analytics Marts]
    F --> G[BI Tools/Analysis]
```

## ğŸ“Š Key Metrics

- Data Volume: 5.3+ million rows (11,000+ stocks Ã— 500+ trading days)
- Daily Processing: ~11,000 new records
- Update Frequency: Daily at market close + 1 day
- Russell 3000 Coverage: ~85% (2,500+ active constituents)
- Historical Depth: 2+ years of trading data
- Pipeline Runtime: ~15 minutes end-to-end

## ğŸ› ï¸ Technology Stack

- Orchestration: Apache Airflow 2.8+
- Data Warehouse: Google BigQuery
- Transformation: dbt Core 1.10.11
- Data Source: Polygon.io API
- Containerization: Docker & Docker Compose
- Language: Python 3.10+, SQL

## ğŸ“ Project Structure

```text
stock_market_pipeline/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                    # Airflow DAG definitions
â”‚   â”‚   â””â”€â”€ market_data_pipeline_dag.py
â”‚   â”œâ”€â”€ config/                  # Airflow configuration
â”‚   â””â”€â”€ plugins/                 # Custom operators/sensors
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ stock_analytics/
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ staging/         # Raw data cleaning layer
â”‚       â”‚   â”œâ”€â”€ intermediate/    # Russell 3000 filtered data
â”‚       â”‚   â””â”€â”€ marts/           # Analytics-ready tables
â”‚       â”œâ”€â”€ macros/              # Reusable SQL functions
â”‚       â”œâ”€â”€ seeds/               # Russell 3000 constituent lists
â”‚       â””â”€â”€ tests/               # Data quality tests
â””â”€â”€ src/
    â”œâ”€â”€ bigquery_client.py            # BigQuery operations
    â”œâ”€â”€ extraction.py                 # Polygon API interface
    â”œâ”€â”€ extract_load_polygon_data.py  # Main ETL logic
    â”œâ”€â”€ config.py                     # Environmental variables loading logic
    â”œâ”€â”€ load.py                       # Logic to load data to BigQuery
    â””â”€â”€ utils.py                      # Misc. utility logic

```

## ğŸ¯ Data Models

### Staging Layer

- `stg_daily_stocks` - Cleaned and typed raw market data (11k stocks)
- `stg_russell_3000__constituents` - Index membership with validity dates

### Intermediate Layer

- `int_russell3000__daily` - Filtered to Russell 3000 with enrichments (1.3M rows)

### Analytics Marts
#### `fct_trading_momentum` (1.3M+ rows)
Daily trading signals and technical indicators including:

- Simple Moving Averages (20, 50, 200-day)
- Relative Strength Index (RSI)
- Golden/Death Cross signals
- Volume metrics and 52-week highs/lows

#### `agg_daily_market_breadth` (~500 rows)
Market-wide health indicators:

- Advance/Decline ratios
- Percentage of stocks above key moving averages
- New highs/lows index
- Market momentum indicators

#### `dim_securities_current` (~2,500 rows)
Latest snapshot per ticker with:

- Current technical indicators
- Performance metrics (1w, 1m, 3m, YTD returns)
- Sector comparisons
- Volatility measurements

## ğŸš€ Getting Started
### Prerequisites
- Docker & Docker Compose
- Google Cloud Platform account with BigQuery enabled
- Polygon.io API key
- 10GB+ free disk space

#### Installation
1. Clone the repository
```bash
git clone https://github.com/tmoore-prog/stock_market_pipeline.git
cd stock_market_pipeline
```
2. Set up environmental variables
```bash
cp .env.example .env
cp docker-compose.example.yaml docker-compose.yaml
# Edit both with your credentials:
# - POLYGON_API_KEY
# - GCP_PROJECT_ID
# - POLYGON_API_KEY
# - USER_NAME for Airflow UI sign-in
```
3. Configure BigQuery credentials
```bash
# Place your service account JSON in:
keys/big-query.json
```
4. Build and start services
```bash
docker compose up -d
```
5. Initialize dbt
```bash
cd dbt/stock_analytics
dbt deps # Install test extension packages
dbt seed # Load Russell 3000 constiuent data
```
6. Run initial historic load
```bash
# Access Airflow UI at http://localhost:8080
# Inspect airflow-apiserver logs to find Airflow UI login password
# Trigger: market_data_pipeline DAG
# This will load 2 years of historical data on first run
# Duration: ~3 hours
```
7. Enable daily pipeline
```bash
# Enable market_data_pipeline DAG
# Runs daily at 1200 EST while container is active
```

## ğŸ”„ Pipeline Workflow
### Daily Pipeline (market_data_pipeline)

1. Extract: Fetch previous trading day data from Polygon API
2. Load: Insert into BigQuery raw layer with checkpoint tracking
3. Transform: Run dbt models incrementally
4. Test: Execute data quality tests

### Key Features

- Checkpoint Recovery: Automatic backfill of missing dates
- Incremental Processing: Only processes new/changed data
- Data Quality Tests: 10+ custom tests ensure data integrity
- Point-in-Time Accuracy: Handles Russell 3000 rebalancing

## ğŸ§ª Testing
```bash
cd dbt/stock_analytics
dbt test # Runs all tests

dbt test --select fct_trading_momentum # Example of running specific model tests
```
### Customer singular dbt tests include:
- RSIrange validation (0-100)
- Golden/Death Corss mutual exclusivity
- Advances/Declines/Unchanged totals reconciliation

## ğŸ“Š Data Quality
- Validation: Automatic detection of impossible price movements
- Freshness Checks: Alerts if data is >2 days old <-wip
- Completeness: Monitors for missing tickers or trading days
- Consistency: Validates technical indicator calculations

## ğŸ¨ dbt Documentation
```bash
# Generate and serve dbt documentation
cd dbt/stock_analytics
dbt docs generate
dbt docs serve
# Visit http://localhost:8080 to inspect
# Note: Must be done while Airflow container/Airflow UI is not running
```

## ğŸš¦ Monitoring
- Airflow UI: http://localhost:8080
- BigQuery Console: Monitor query performance and costs
- dbt Artifacts: Execution logs in `target/` directory

## ğŸ’¡ Example Queries (BigQuery Console)
```sql
-- Find most recent golden crosses
SELECT ticker, company, sector
FROM `analytics.fct_trading_momentum`
WHERE trade_date = (SELECT MAX(trade_date) FROM `analytics.fct_trading_momentum`)
  AND golden_cross = 1;

-- Market breadth analysis
SELECT 
  trade_date,
  normalized_ad_ratio,
  market_rsi,
  CASE 
    WHEN pct_market_over_sma50 > 0.8 THEN 'Strong Bullish'
    WHEN pct_market_over_sma50 < 0.2 THEN 'Strong Bearish'
    ELSE 'Neutral'
  END as market_sentiment
FROM `analytics.agg_daily_market_breadth`
WHERE trade_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY);

-- Top performing stocks by sector
SELECT 
  sector,
  ticker,
  return_1m,
  outperformance_vs_sector
FROM `analytics.dim_securities_current`
WHERE performance_percentile > 0.9
ORDER BY sector, return_1m DESC;
```

## ğŸ”§ Configuration
### Adjusting Pipeline Schedule
Edit `airflow/dags/,arket_data_piple_dag.py`:
```python
schedule='0 12 * * 2-6' # This runs daily, Tuesday-Saturday at 1200, extracts data from previous trading day thru sub 2 years
```
### Modifying Technical Indicators
Edit macros in `dbt/stock_analytics/macros/`:

- `calculate_sma.sql` - Adjust moving average periods
- `calculate_return.sql` - Modify return calculations

## ğŸ› Troubleshooting
| Issue | Solution |
|---|---|
| Protobuf errors | Run `pip install --upgrade --force-reinstall protobuf` |
| Missing yesterday_close | Full refresh incremental models |
| Rate limit hit | Check Polygon API quota, readjust sleep timers |
| Memory issues | Increase Docker memory allocation |

## ğŸ“ˆ Performance Optimizations
- Partitioned Tables: Daily partitions on trade_date
- Clustered by Ticker: Optimizes ticker-specific queries
- Incremental Models: Process only new data
- Lookback Windows: 4-7 day windows for late-arriving data

## ğŸš€ Future Enhancements
 - Streamlit dashboard for visualization
 - Machine learning features mart
 - Options data integration
 - Real-time streaming pipeline
 - Kubernetes deployment
 - Data quality alerting via Slack

## ğŸ“ License
### MIT License - see LICENSE file for details

## ğŸ¤ Contributing
1. Fork the repository
2. Create a feature branch (git checkout -b feature/amazing-feature)
3. Commit changes (git commit -m 'Add amazing feature')
4. Push to branch (git push origin feature/amazing-feature)
5. Open a Pull Request

## ğŸ‘¤ Author
Trent Moore

GitHub: @tmoore-prog
LinkedIn: Trent Moore (https://www.linkedin.com/in/trent-moore-64abb4115)

## ğŸ™ Acknowledgments

- Polygon.io for market data API
- dbt Labs for the transformation framework
- Apache Airflow community
- iShares Investments for index constituent data
